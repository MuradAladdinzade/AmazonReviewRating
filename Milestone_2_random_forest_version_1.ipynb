{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "395bde9c-73c9-455c-b50f-2fbb29b1df9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "Requirement already satisfied: pyspark in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5ec586f9-1c33-42db-8861-6bc16c88550d/lib/python3.10/site-packages (3.5.5)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5ec586f9-1c33-42db-8861-6bc16c88550d/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n",
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5659e118-fe2f-40b0-81bf-80c1b13b3a63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, length, count, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5a46a61-85d0-44c6-a2b7-e00e9d54b785",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set the access key for your Azure Blob Storage account\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.aladdimbigdata.blob.core.windows.net\", \n",
    "    \"accesskey_removed\"  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3a40cd6-1f74-494e-afc2-281f8093b0ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      "\n",
      "+----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+-------------+------------------------------------------------------------+\n",
      "|asin      |reviewText                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |overall|category     |summary                                                     |\n",
      "+----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+-------------+------------------------------------------------------------+\n",
      "|0151004714|This is the best novel I have read in 2 or 3 years.  It is everything that fiction should be -- beautifully written, engaging, well-plotted and structured.  It has several layers of meanings -- historical, family,  philosophical and more -- and blends them all skillfully and interestingly.  It makes the American grad student/writers' workshop \"my parents were  mean to me and then my professors were mean to me\" trivia look  childish and silly by comparison, as they are.\\nAnyone who says this is an  adolescent girl's coming of age story is trivializing it.  Ignore them.  Read this book if you love literature.\\nI was particularly impressed with  this young author's grasp of the meaning and texture of the lost world of  French Algeria in the 1950's and '60's...particularly poignant when read in  1999 from another ruined and abandoned French colony, amid the decaying  buildings of Phnom Penh...\\nI hope the author will write many more books  and that her publishers will bring her first novel back into print -- I  want to read it.  Thank you, Ms. Messud, for writing such a wonderful work.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |5.0    |Electronics_5|A star is born                                              |\n",
      "|0151004714|Pages and pages of introspection, in the style of writers like Henry James.  I like this kind of  novels and the writer occasionally delights me with her descriptions and observations.  But it's way too repetitious for me and, I think, some parts could have been cut out while still preserving, and probably more tightly crystallizing, the themes and \"truths\" within the story.\\n\\nIt's a story I could relate to but I wish it hadn't been too tedious to read.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |3.0    |Electronics_5|A stream of consciousness novel                             |\n",
      "|0151004714|This is the kind of novel to read when you have time to lose yourself in a book for days, possibly weeks. I had to go back and reread it as soon as I finished it because it is so rich in historical and psychological  details, particularly themes centering on painful family dynamics and history.\\n\\nPotential readers of this novel by Claire Messud should know it isn't exactly a sunny, bouncy work. If you are looking for that you'll want to look elsewhere. It also moves between countries, including sections based in France, the U.S. and Algeria.\\n\\nThe focus of the novel is the LaBasse family and the story is told from the viewpoint of Sagesse, a woman who conveys the weight of a turbulent family history. Starting with Sagesse's grandfather, a man who owns the Hotel Bellevue in France,the family dynamics begin to play themselves out, connecting to larger themes of France and Algeria's history as well, with stunning consequences.\\n\\nSagesse's parents, Alex and Carol, reveal plenty about the kind of problems that can arise when there are cross-cultural challenges to face in a marriage. Carol, coming from America, marries Alex, but is unable to see the potential sacrifices and problems she'll face as she moves overseas to live with her husband...or the impact the marriage will have on her daughter and the rest of the family, including a handicapped son.\\n\\n It is a bit hard to categorize this book because it covers so many areas. It is part mystery because Sagesse is trying to separate truth from myth when figuring out her family's history. It is part suspense because of Sagesse's short love affair and a moment of violence (and I don't want to give the details of that moment or I'll spoil it for you). It is also a historical novel, rich with information about events in Algeria and France and the United States.\\n\\nFor me, also, this one centers on themes of personal identity and displacement,primarily for Sagesse, a woman trying to figure out where she belongs in the world and how to make peace with her difficult family as well as coming to grips with history's impact on her family.                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |5.0    |Electronics_5|I'm a huge fan of the author and this one did not disappoint|\n",
      "|0151004714|What gorgeous language! What an incredible writer! The Last Life is one of  the best written novels I have ever read. This incredible novel contains  various elements of fiction: historical, philosophical, and just the right  touch of magical realism.\\nThe story is about a French-Algerian family  and the constant moving to different parts of the world -- the South of  France, the East Coast of the U.S. and Algeria -- has affected each of  their lives. It seems as though each family member has found his or her  identity in one of the many places they have been. As a result, they never  feel like they have found a home. The thing that struck me most when  reading this book was how the author described life in the lost world of  French Algeria in the 1950's. The historical part of the novel is  incredibly accurate. Let me tell you that I have learned a lot of important  parts of history in this book. I also love the fact that the story is told  through the eyes of fourteen-year-old Lagesse, seen through the eyes of  innocence indeed!  The Last Life is -- without a  stretch of doubt -- the best historical fiction novel I have ever read.  This novel should not be overlooked. Run along and get it!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |5.0    |Electronics_5|The most beautiful book I have ever read!                   |\n",
      "|0151004714|I was taken in by reviews that compared this book with The Leopard or promised a bildungsroman set in a family that is isolated and on the move.  Well...I didn't feel any parallel with The Leopard and the author overstuffs her story with so many events that isolation is lost in the opportunity for  so many subplots.  And who's moving - in any sense - in this novel?  Where's the \"bildung?\"  The denouement?  The dsillusionner?  I must have  skimmed the wrong paragraph.\\nI do agree that parts of the novel are very  engaging.  They are.  And, like some other readers, I did enjoy the events  in Algeria.  This is the strongest part of the novel, perhaps because it is  not so deeply stuck in the protagonist's life that had the unfortunate  knack of boring me in the first-person!  (Yeah, that's what I mean.)  Overall, the novel is just a bit overdone.  I prefer a bildungsroman - if  that's what this should be - to show me a growth, not tell me all about the  all-too-interesting things that happened along the way.\\nIf you're the  kind of person who can sit for hours and listen to a friend tell you  endless stories about herself that you can't help but suspect are just a  tad trumped up, then you might really dig into this novel.  Otherwise you  might have an experience more like mine which was one of detached interest.  For my part, I was relieved that I could close the book whenever I wanted  to quiet my jabbery friend.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |3.0    |Electronics_5|A dissenting view--In part.                                 |\n",
      "|0380709473|I read this probably 50 years ago in my youth and I just re-read it for the first time.  It was a fun read but not as good as my memory of it.\\n\\nBarry                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |4.0    |Electronics_5|Above average mystery                                       |\n",
      "|0380709473|I read every Perry mason book voraciously. Finding the Lam/Cool mysteries, and getting to enjoy more of Gardner's characters, is like Christmas morning as a kid. Each book is a gift!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |5.0    |Electronics_5|Lam is cool!                                                |\n",
      "|0380709473|I love this series of Bertha and Lamb..  Great novels.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |5.0    |Electronics_5|Five Stars                                                  |\n",
      "|0380709473|Great read!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |5.0    |Electronics_5|Five Stars                                                  |\n",
      "|0380709473|Crows Can't Count, A.A. Fair\\n\\nMr. Harry Sharples came to the Cool & Lam Agency; he was a co-trustee for a trust fund that will terminate when the youngest beneficiary turns 25. One of the heirs, Robert Hockley, is wild and gambles. The other, Shirley Bruce, isn't but refuses to take more than the other. A large part of the estate is in Colombian emerald mines. A jeweler has an emerald pendant for sale. Sharples believes this was part of the estate given to Shirley Bruce. Sharples wants Lam to find out how the jeweler got the pendant, and if any pressure was used (Chapter 1). Chapter 2 describes the preliminary investigation. Lam sees Robert Cameron, the other co-trustee show up. Sharples is surprised and astonished to hear this news (Chapter 3). When Lam and Sharples go to visit Cameron, they find a dead body. They call the police from outside the house (Chapter 4). Sharples tells Sgt. Sam Buda that Cameron had no enemies (Chapter 5). The emerald pendant was there. The police investigate the jeweler and the stockbroker who was visited by Robert Cameron earlier that day (Chapter 8).\\n\\nLam visits Dona Grafton, who minded Cameron's crow when Cameron left the country (Chapter 11). An anonymous donor sent Dona a box of chocolates. Lam inspects the crow's nest. Lam learns that Robert Hockley was getting a passport and decides to follow him (Chapter 12). The police bring Lam to Sharples' residence. His office is disordered as from a struggle, and Sharples is missing (Chapter 15). Lam meets George Prenter on the airplane, and hears of the wonderful life and climate in Medellin Colombia (Chapter 16). A visitor surprises Lam and we learn the true facts behind the earlier events (Chapter 17). Lam learns that someone else has been killed (Chapter 19).\\n\\nLam and Cool get a telephone call about a prison escape. They will be guarded (Chapter 21). Lam and \"the delightful Senora Cool\" will be allowed to return to California as soon as possible (Chapter 22). Then Lam visits Dona Grafton and asks questions (Chapter 23). There is amazing testimony from a witness in Chapter 24. Lam places a call to the police so they will get an interpreter and notary public. This written statement will convict a businessman of fraud, and affect the wealth in a trust. Lam explains the facts behind the scandal (Chapter 25). [Do you remember the story of King Solomon and the disputed child?] The pieces of the jigsaw puzzle fall into place, just as the apple falls close to its tree (Chapter 26). One benefit of this story is the description of the trip to Colombia.|4.0    |Electronics_5|A Fast and Far Moving Adventure                             |\n",
      "+----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+-------------+------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create or get your SparkSession\n",
    "spark = SparkSession.builder.appName(\"AmazonReviewsEDA\").getOrCreate()\n",
    "\n",
    "\n",
    "data_path = \"wasbs://datafiles@aladdimbigdata.blob.core.windows.net/*.csv\"\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .option(\"escape\", '\"') \\\n",
    "    .csv(data_path)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73358819-da1a-4099-bbfe-49a329a1d839",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Dataset Overview\n",
    "\n",
    "The dataset consists of Amazon product reviews across multiple categories with the following columns:\n",
    "\n",
    "| Column Name | Data Type          | Description                                            | \n",
    "|-------------|--------------------|--------------------------------------------------------|\n",
    "|asin         | String             | Unique Product ID                                      |\n",
    "|reviewText   | String             | Full customer review text                              |\n",
    "|overall      | Float (converted)  | Star rating (1 to 5)                                   |\n",
    "|category     | String             | Product category (e.g., Beauty, Fashion, Appliances)   |\n",
    "|summary      | String             | Brief review summary                                   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75b6dbb8-390c-4c6d-85df-d904e7cec85a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Columns: 5\n",
      "Columns: ['asin', 'reviewText', 'overall', 'category', 'summary']\n",
      "Total Rows: 18637932\n",
      "Total Categories: 13\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Columns: {len(df.columns)}\")\n",
    "print(f\"Columns: {df.columns}\")\n",
    "print(f\"Total Rows: {df.count()}\")\n",
    "print(f\"Total Categories: {df.select('category').distinct().count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a910439d-6497-4929-a5c1-8a6ca9eeb1f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Missing Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a00d4228-e678-486c-8173-cb89bcd772e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-------+--------+-------+\n",
      "|asin|reviewText|overall|category|summary|\n",
      "+----+----------+-------+--------+-------+\n",
      "|   0|         0|      0|       0|      0|\n",
      "+----+----------+-------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count NULLs for each column\n",
    "missing_values = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7815769-c29b-4049-be16-cd2d3a081bbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|overall|   count|\n",
      "+-------+--------+\n",
      "|    1.0| 1003006|\n",
      "|    2.0|  749579|\n",
      "|    3.0| 1393843|\n",
      "|    4.0| 3102498|\n",
      "|    5.0|12389006|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"overall\").count().orderBy(\"overall\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "595c1cd8-79f0-4f8a-bc5e-238f1b8c17f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|overall|   count|\n",
      "+-------+--------+\n",
      "|    1.0| 1003006|\n",
      "|    2.0|  749579|\n",
      "|    3.0| 1393843|\n",
      "|    4.0| 3102498|\n",
      "|    5.0|12389006|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where 'overall' is 0.0\n",
    "df = df.filter(col(\"overall\") != 0.0)\n",
    "\n",
    "# Verify that 0.0 ratings are removed\n",
    "df.groupBy(\"overall\").count().orderBy(\"overall\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38d35a17-5327-4296-8593-901e4701d9d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Balancing the dataset \n",
    "\n",
    "As seen above the number of rows and everything do not match at all. As a result it is highly skewed to be spositive \n",
    "\n",
    "Down sampling can be done in order ot help with this skewed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b96379e2-9ce3-4ab8-9ac7-9dec319e5303",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|overall| count|\n",
      "+-------+------+\n",
      "|    1.0|749647|\n",
      "|    2.0|749579|\n",
      "|    3.0|750397|\n",
      "|    4.0|750987|\n",
      "|    5.0|749067|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Sample size per class\n",
    "target_size = 749579\n",
    "\n",
    "# Cache original df to avoid recomputation\n",
    "df = df.cache()\n",
    "\n",
    "# Get all class counts in one shot\n",
    "class_counts = df.groupBy(\"overall\").count().collect()\n",
    "\n",
    "# Precompute fractions\n",
    "sampling_map = {row[\"overall\"]: target_size / row[\"count\"] for row in class_counts}\n",
    "\n",
    "# Create sampled DataFrames list\n",
    "sampled_dfs = []\n",
    "\n",
    "for rating, fraction in sampling_map.items():\n",
    "    sampled = df.filter(col(\"overall\") == rating).sample(withReplacement=False, fraction=fraction, seed=42)\n",
    "    sampled_dfs.append(sampled)\n",
    "\n",
    "# Merge all sampled DataFrames efficiently\n",
    "balanced_df = sampled_dfs[0]\n",
    "for df_part in sampled_dfs[1:]:\n",
    "    balanced_df = balanced_df.unionByName(df_part)\n",
    "\n",
    "# ✅ Show final class distribution\n",
    "balanced_df.groupBy(\"overall\").count().orderBy(\"overall\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0559156-6b59-491b-a5b0-6dcf6e9e582b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|    review_length|\n",
      "+-------+-----------------+\n",
      "|  count|          3749677|\n",
      "|   mean|363.3758283713504|\n",
      "| stddev|580.9268160841771|\n",
      "|    min|                1|\n",
      "|    max|            32632|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add review length column\n",
    "balanced_df = balanced_df.withColumn(\"review_length\", length(col(\"reviewText\")))\n",
    "\n",
    "# Get summary statistics\n",
    "balanced_df.select(\"review_length\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af850c31-a67b-4cb2-a9a8-713e8a306d71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|       mean_rating|        std_rating|\n",
      "+------------------+------------------+\n",
      "|3.0000661390301087|1.4138429562969868|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "balanced_df.selectExpr(\"mean(overall) as mean_rating\", \"stddev(overall) as std_rating\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7234f2f-5734-4285-8898-b417ffe7cef7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Goals\n",
    "\n",
    "Our goal is to predict the product rating (overall) based on the review text (reviewText).\n",
    "\n",
    "| Column     | Role            |\n",
    "|------------|-----------------|\n",
    "| reviewText | Feature (Input) |\n",
    "| overall    | Target (Output) |\n",
    "\n",
    "Since overall is an ordinal variable (1-5 stars), we can treat this problem in two ways:\n",
    "\n",
    "1. Classification: Predict one of 5 classes (1, 2, 3, 4, or 5 stars).\n",
    "2. Regression: Predict a continuous rating (e.g., 3.8) and round to the nearest star.\n",
    "\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "The choice of metric depends on whether we treat the task as classification or regression.\n",
    "\n",
    "### Classification Metrics (if treating as a multi-class problem)\n",
    "\n",
    "If we classify reviews into 5-star categories, we use:\n",
    "\n",
    "1. Accuracy: Measures how often the model correctly predicts the exact rating.\n",
    "    - Accuracy = Correct Predictions / Total Predictions\n",
    "\n",
    "2. Precision, Recall, and F1-Score: Useful if we want balanced predictions across all ratings.\n",
    "    - Precision = True Positives / True Positives + False Positives\n",
    "    - Recall =  True Positives / True Positives + False Negatives\n",
    "    - F1-Score = 2 × Precision × Recall / Precision + Recall\n",
    "\n",
    "### Regression Metrics (if treating as a continuous problem)\n",
    "\n",
    "If we predict ratings as continuous values, we use:\n",
    "\n",
    "1. Mean Squared Error (MSE): Penalizes large errors more than small ones.\n",
    "    - MSE = (1/N) * Σ (yi - ŷi)²\n",
    "2. Root Mean Squared Error (RMSE): More interpretable because it has the same unit as the target variable.\n",
    "    - RMSE = sqrt(MSE)\n",
    "3. Mean Absolute Error (MAE): Measures the average absolute difference between predicted and actual ratings.\n",
    "    - MAE = (1/N) * Σ |yi - ŷi|\n",
    "4. R² Score (Coefficient of Determination): Measures how well predictions match the actual ratings.\n",
    "    - R² = 1 - (Σ (yi - ŷi)² / Σ (yi - ȳ)²)\n",
    "\n",
    "\n",
    "## Choosing the Right Approach\n",
    "\n",
    "| Approach                             | Pros                          | Cons                                                             | \n",
    "|--------------------------------------|-------------------------------|------------------------------------------------------------------|\n",
    "| Classification (1,2,3,4,5 stars)     | Simple, interpretable, direct | Ignores the ordinal nature of ratings                            |\n",
    "| Regression (predict rating as float) | Captures ordinal structure    | Harder to optimize, may predict invalid values (e.g., 3.8 stars) |\n",
    "\n",
    "### Best Choice?\n",
    "Since ratings are ordinal, we can try both approaches and compare results.\n",
    "\n",
    "## Data Ingestion\n",
    "\n",
    "Since we have 39 CSV files spread across multiple categories, we need an efficient ingestion pipeline.\n",
    "\n",
    "### PySpark Pipeline for Loading Data\n",
    "1. Read all CSV files from a directory\n",
    "2. Infer schema & ensure consistent column types\n",
    "3. Combine files into a single DataFrame\n",
    "4. Store in an optimized format (Parquet, Delta, etc.) for fast access\n",
    "\n",
    "## Handling multiple Data Files\n",
    "\n",
    "Since we are working with Amazon product reviews, we primarily have one main dataset consisting of reviews across multiple categories. We have concatted rows and that's all we need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e631af8-cc12-465e-9a4e-7c2483d9fe93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b38c1189-4352-49be-9ba4-9d3c484b5ead",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Checkpointing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be1b0245-e5f9-4d5b-83a0-59af4c8bd993",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Define checkpoint path\n",
    "# checkpoint_path = \"./amazon_reviews_checkpoint\"\n",
    "\n",
    "# # Save as Parquet (compressed format)\n",
    "# df.write.mode(\"overwrite\").parquet(checkpoint_path)\n",
    "# # df.write.mode(\"overwrite\").option(\"compression\", \"none\").parquet(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fe6eae2-319b-4f1a-9f90-2740b23817a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Splitting Strategy\n",
    "\n",
    "Since our dataset consists of Amazon product reviews, we need to ensure that:\n",
    "\n",
    "1. Stratified sampling is used to maintain the rating distribution across all splits.\n",
    "2. No product (asin) appears in both train and test sets to prevent leakage.\n",
    "\n",
    "### Split Ratio\n",
    "\n",
    "| Set            | Purpose                       | Size |\n",
    "|----------------|-------------------------------|------|\n",
    "| Training Set   | Used to train the model       | 70%  |\n",
    "| Validation Set | Used to tune hyperparameters  | 15%  |\n",
    "| Test Set       | Used for final evaluation     | 15%  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea9ea475-b4e2-4fa6-9009-1369b3c91eb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Rows: 2626465\n",
      "Validation Rows: 561074\n",
      "Test Rows: 562138\n",
      "+-------+------+\n",
      "|overall| count|\n",
      "+-------+------+\n",
      "|    1.0|524954|\n",
      "|    2.0|525178|\n",
      "|    3.0|525650|\n",
      "|    4.0|526004|\n",
      "|    5.0|524679|\n",
      "+-------+------+\n",
      "\n",
      "+-------+------+\n",
      "|overall| count|\n",
      "+-------+------+\n",
      "|    1.0|112190|\n",
      "|    2.0|112134|\n",
      "|    3.0|112244|\n",
      "|    4.0|112416|\n",
      "|    5.0|112090|\n",
      "+-------+------+\n",
      "\n",
      "+-------+------+\n",
      "|overall| count|\n",
      "+-------+------+\n",
      "|    1.0|112503|\n",
      "|    2.0|112267|\n",
      "|    3.0|112503|\n",
      "|    4.0|112567|\n",
      "|    5.0|112298|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def stratified_split(df: DataFrame, label_col: str, weights: list, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Stratified train/val/test split for a classification dataset in Spark.\n",
    "    Returns (train_df, val_df, test_df)\n",
    "    \"\"\"\n",
    "    train_df = val_df = test_df = None\n",
    "\n",
    "    # Loop through each class and split separately\n",
    "    for label in [1.0, 2.0, 3.0, 4.0, 5.0]:\n",
    "        class_df = df.filter(col(label_col) == label)\n",
    "        splits = class_df.randomSplit(weights, seed=seed)\n",
    "\n",
    "        train_df = splits[0] if train_df is None else train_df.unionByName(splits[0])\n",
    "        val_df   = splits[1] if val_df is None   else val_df.unionByName(splits[1])\n",
    "        test_df  = splits[2] if test_df is None  else test_df.unionByName(splits[2])\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# ✅ Use the function on balanced_df\n",
    "train_df, val_df, test_df = stratified_split(balanced_df, label_col=\"overall\", weights=[0.7, 0.15, 0.15])\n",
    "\n",
    "# ✅ Verify counts\n",
    "print(f\"Training Rows: {train_df.count()}\")\n",
    "print(f\"Validation Rows: {val_df.count()}\")\n",
    "print(f\"Test Rows: {test_df.count()}\")\n",
    "\n",
    "# ✅ Verify class distribution\n",
    "train_df.groupBy(\"overall\").count().orderBy(\"overall\").show()\n",
    "val_df.groupBy(\"overall\").count().orderBy(\"overall\").show()\n",
    "test_df.groupBy(\"overall\").count().orderBy(\"overall\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a81b452b-8df5-4a35-b53f-ed94614895db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Preventing Data Leakage\n",
    "\n",
    "To avoid data leakage, we must ensure that:\n",
    "- No product (asin) appears in both Train & Test.\n",
    "- Text normalization is applied only using Training data statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23efbe54-0963-4b87-adcb-5393f09b510f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get unique ASINs from the training set\n",
    "train_asins = train_df.select(\"asin\").distinct()\n",
    "\n",
    "# Ensure validation & test sets do not contain ASINs from train set\n",
    "# Why Use left_anti Join? - Removes any products (asin) in train_df from val_df and test_df.\n",
    "val_df = val_df.join(train_asins, on=\"asin\", how=\"left_anti\")\n",
    "test_df = test_df.join(train_asins, on=\"asin\", how=\"left_anti\")\n",
    "\n",
    "# Verify final counts\n",
    "# print(f\"Final Validation Rows: {val_df.count()}\")\n",
    "# print(f\"Final Test Rows: {test_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55663147-3355-4d86-a87c-d5a5510b13cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Normalizing Data Using Training Statistics\n",
    "\n",
    "Since review lengths and text embeddings may have large variations, we normalize:\n",
    "- Review length → Standardize using mean & std from training data.\n",
    "- Text embeddings (TF-IDF, Word2Vec, etc.) → Fit only on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d118736-33e7-4e4f-8050-d60e7537264b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, stddev\n",
    "\n",
    "# Compute mean & std dev on training data\n",
    "stats = train_df.selectExpr(\"mean(review_length) as mean_length\", \"stddev(review_length) as std_length\").collect()[0]\n",
    "\n",
    "mean_length = stats[\"mean_length\"]\n",
    "std_length = stats[\"std_length\"]\n",
    "\n",
    "# Normalize review length\n",
    "train_df = train_df.withColumn(\"review_length_normalized\", (col(\"review_length\") - mean_length) / std_length)\n",
    "val_df = val_df.withColumn(\"review_length_normalized\", (col(\"review_length\") - mean_length) / std_length)\n",
    "test_df = test_df.withColumn(\"review_length_normalized\", (col(\"review_length\") - mean_length) / std_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ec50f98-21a6-4e75-9f9d-eb68b5e6c59f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cb_base_path = \"wasbs://datafiles@aladdimbigdata.blob.core.windows.net/catboost_data\"\n",
    "\n",
    "catboost_cols = [\"reviewText\", \"review_length_normalized\", \"category\", \"overall\"]\n",
    "\n",
    "train_df.select(*catboost_cols) \\\n",
    "    .write.mode(\"overwrite\").parquet(f\"{cb_base_path}/train\")\n",
    "\n",
    "val_df.select(*catboost_cols) \\\n",
    "    .write.mode(\"overwrite\").parquet(f\"{cb_base_path}/val\")\n",
    "\n",
    "test_df.select(*catboost_cols) \\\n",
    "    .write.mode(\"overwrite\").parquet(f\"{cb_base_path}/test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb3688cd-2025-44fb-8da3-3e1eee6f14f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### EDA and Handling Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ba3e2c5-e967-49b8-a0b5-b6748eead5ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8ff9835-b155-42ab-b260-6fa3959a7b3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# Drop existing 'category_index' and 'category_onehot' columns if they exist\n",
    "train_df = train_df.drop(\"category_index\", \"category_onehot\")\n",
    "val_df = val_df.drop(\"category_index\", \"category_onehot\")\n",
    "test_df = test_df.drop(\"category_index\", \"category_onehot\")\n",
    "\n",
    "# Convert categorical 'category' column to numerical using StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"category_index\")\n",
    "encoder = OneHotEncoder(inputCol=\"category_index\", outputCol=\"category_onehot\")\n",
    "\n",
    "# Fit the indexer on the training data and then transform it\n",
    "indexer_model = indexer.fit(train_df)\n",
    "train_df = indexer_model.transform(train_df)\n",
    "\n",
    "# Fit the encoder on the training data and then transform it\n",
    "encoder_model = encoder.fit(train_df)\n",
    "train_df = encoder_model.transform(train_df)\n",
    "\n",
    "# Apply the same transformations to validation and test sets using the fitted models\n",
    "val_df = indexer_model.transform(val_df)\n",
    "val_df = encoder_model.transform(val_df)\n",
    "\n",
    "test_df = indexer_model.transform(test_df)\n",
    "test_df = encoder_model.transform(test_df)\n",
    "\n",
    "# Verify the transformation\n",
    "# train_df.select(\"category\", \"category_index\", \"category_onehot\").show(5)\n",
    "# val_df.select(\"category\", \"category_index\", \"category_onehot\").show(5)\n",
    "# test_df.select(\"category\", \"category_index\", \"category_onehot\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e822aa1-ad29-463b-8678-b7e7e7c309e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the training dataset: ['asin', 'reviewText', 'overall', 'category', 'summary', 'review_length', 'review_length_normalized', 'category_index', 'category_onehot']\n"
     ]
    }
   ],
   "source": [
    "# Check the column names to ensure 'category' exists\n",
    "print(\"Columns in the training dataset:\", train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea609b31-7ce9-4d58-962c-2b2206c61a9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the validation dataset: ['asin', 'reviewText', 'overall', 'category', 'summary', 'review_length', 'review_length_normalized', 'category_index', 'category_onehot']\n"
     ]
    }
   ],
   "source": [
    "# Check the column names to ensure 'category' exists\n",
    "print(\"Columns in the validation dataset:\", val_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ba2f0a1-fd2a-4aa9-b206-23e5d2047244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the validation dataset: ['asin', 'reviewText', 'overall', 'category', 'summary', 'review_length', 'review_length_normalized', 'category_index', 'category_onehot']\n"
     ]
    }
   ],
   "source": [
    "# Check the column names to ensure 'category' exists\n",
    "print(\"Columns in the validation dataset:\", test_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c039e84a-e1da-4502-a5e8-36802d9642c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec6ec3a4-bac2-4b71-b188-eb7a3884ee8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Text Feature Engineering \n",
    "\n",
    "This section implements a text preprocessing pipeline for our product review classification task. The pipeline handles:\n",
    "\n",
    "1. **Data Cleaning**: Filling missing values in review text and summaries to prevent null errors\n",
    "2. **Column Management**: Removing any conflicting columns from previous runs\n",
    "3. **Tokenization**: Converting the raw review text into word tokens for feature extraction\n",
    "\n",
    "This is the first part of our feature engineering process, which will be followed by:\n",
    "- Feature extraction using TF-IDF\n",
    "- Category encoding\n",
    "- Feature assembly into vectors for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27152db0-a7d0-4a71-baed-5b5dc29fe84b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Handle Missing Data (Optional)\n",
    "train_df = train_df.fillna({'reviewText': 'Unknown', 'summary': 'Unknown'})\n",
    "val_df = val_df.fillna({'reviewText': 'Unknown', 'summary': 'Unknown'})\n",
    "test_df = test_df.fillna({'reviewText': 'Unknown', 'summary': 'Unknown'})\n",
    "\n",
    "# Step 2: Drop any existing columns that might conflict\n",
    "train_df = train_df.drop(\"words\", \"raw_features\", \"features\", \"category_index\", \"category_onehot\")\n",
    "val_df = val_df.drop(\"words\", \"raw_features\", \"features\", \"category_index\", \"category_onehot\")\n",
    "test_df = test_df.drop(\"words\", \"raw_features\", \"features\", \"category_index\", \"category_onehot\")\n",
    "\n",
    "# Step 3: Tokenize the 'reviewText' column into words\n",
    "tokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"words\")\n",
    "train_df = tokenizer.transform(train_df)\n",
    "val_df = tokenizer.transform(val_df)\n",
    "test_df = tokenizer.transform(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20f75d32-8c1c-4395-be55-d5445647886d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|reviewText                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |words                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|As Americans we all know where this is made and how bad they are at making ANYTHING worth buying. Like our sneakers being made so cheaply. and never in our sizes.  or widths. BUT... this little smuck uses up battery life like nothing you have ever seen. It has had a few problems where it would not play the entire list of songs and I had to erase everything and reprogram it. It does not give us the sound or the quality it would had Germans or American workers had made it. It's ok.. but not quality.                              |[as, americans, we, all, know, where, this, is, made, and, how, bad, they, are, at, making, anything, worth, buying., like, our, sneakers, being, made, so, cheaply., and, never, in, our, sizes., , or, widths., but..., this, little, smuck, uses, up, battery, life, like, nothing, you, have, ever, seen., it, has, had, a, few, problems, where, it, would, not, play, the, entire, list, of, songs, and, i, had, to, erase, everything, and, reprogram, it., it, does, not, give, us, the, sound, or, the, quality, it, would, had, germans, or, american, workers, had, made, it., it's, ok.., but, not, quality.]                                    |\n",
      "|I have to say I was sorely disappointed in this mp3 player. I read mixed reviews before purchasing and though well maybe there is a chance I'll get a good one. It's a total gamble for the price. Well,I can't even begin to figure out how to use this thing. It came with no instructions and it's so confusing I don't believe even a tech savy teen could figure this mess out. And it's so cheaply made it looks like it could break in half at any moment. I don't recommend this mp3 player. It's a waste of time and money. Avoid this one.|[i, have, to, say, i, was, sorely, disappointed, in, this, mp3, player., i, read, mixed, reviews, before, purchasing, and, though, well, maybe, there, is, a, chance, i'll, get, a, good, one., it's, a, total, gamble, for, the, price., well,i, can't, even, begin, to, figure, out, how, to, use, this, thing., it, came, with, no, instructions, and, it's, so, confusing, i, don't, believe, even, a, tech, savy, teen, could, figure, this, mess, out., and, it's, so, cheaply, made, it, looks, like, it, could, break, in, half, at, any, moment., i, don't, recommend, this, mp3, player., it's, a, waste, of, time, and, money., avoid, this, one.]|\n",
      "|These are pretty cool mp3 players with some nice features, including FM radio. But they simply do not work and are therefore a waste of money. I bought a two pack and one did not work as soon as it arrived. It would not play music at all and I'm not sure what else may have been wrong with it. I sent it back and while I was waiting for it to be returned, the other one stopped picking up radio stations. Now the seller will not return my messages. I guess I'm stuck with them. BEWARE OF THIS ITEM!                                  |[these, are, pretty, cool, mp3, players, with, some, nice, features,, including, fm, radio., but, they, simply, do, not, work, and, are, therefore, a, waste, of, money., i, bought, a, two, pack, and, one, did, not, work, as, soon, as, it, arrived., it, would, not, play, music, at, all, and, i'm, not, sure, what, else, may, have, been, wrong, with, it., i, sent, it, back, and, while, i, was, waiting, for, it, to, be, returned,, the, other, one, stopped, picking, up, radio, stations., now, the, seller, will, not, return, my, messages., i, guess, i'm, stuck, with, them., beware, of, this, item!]                                      |\n",
      "|This player is awful.  Goes through a battery in just a few hours.  The controls are difficult to use.  I would ship it back, but since I have would have to pay return shipping, it's not worth the trouble.                                                                                                                                                                                                                                                                                                                                       |[this, player, is, awful., , goes, through, a, battery, in, just, a, few, hours., , the, controls, are, difficult, to, use., , i, would, ship, it, back,, but, since, i, have, would, have, to, pay, return, shipping,, it's, not, worth, the, trouble.]                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|i bought this thing months ago and have used it a lot. its crap !!!! im good at navigating ancient devices like this, i find it a challenge. this one takes the cake. it is the worst planned out mp3 op system i have ever seen. the fm radio never gets a clear signal.. and then after one week the little volume board inside is malfunctioning and doing chaotic things like skipping to next song when you press the up vol. button                                                                                                           |[i, bought, this, thing, months, ago, and, have, used, it, a, lot., its, crap, !!!!, im, good, at, navigating, ancient, devices, like, this,, i, find, it, a, challenge., this, one, takes, the, cake., it, is, the, worst, planned, out, mp3, op, system, i, have, ever, seen., the, fm, radio, never, gets, a, clear, signal.., and, then, after, one, week, the, little, volume, board, inside, is, malfunctioning, and, doing, chaotic, things, like, skipping, to, next, song, when, you, press, the, up, vol., button]                                                                                                                                 |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first few rows of the 'words' column to verify tokenization\n",
    "train_df.select(\"reviewText\", \"words\").show(5, truncate=False)\n",
    "# val_df.select(\"reviewText\", \"words\").show(5, truncate=False)\n",
    "# test_df.select(\"reviewText\", \"words\").show(5, truncate=False)\n",
    "\n",
    "# Try selecting a smaller number of rows to debug\n",
    "# val_df.limit(5).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27380783-c0d6-454f-92e5-32ad0867de99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature Extraction with HashingTF\n",
    "\n",
    "In this step, we convert our tokenized text into numerical features using the HashingTF transformer:\n",
    "\n",
    "1. **Term Frequency Calculation**: The HashingTF transformer maps each word to a fixed-size feature vector using a hashing function\n",
    "   \n",
    "2. **Dimensionality Control**: We set `numFeatures=1000` to limit the feature space to 1000 dimensions, balancing between model complexity and computational efficiency\n",
    "   \n",
    "3. **Feature Generation**: This creates the \"raw_features\" column containing term frequency vectors for each review\n",
    "\n",
    "This transformation is a critical step in converting text data into the numerical format required for machine learning algorithms. The term frequency vectors represent how often each word appears in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e89defe-03ec-4722-92ca-635ad2a10ffc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Step 4: Apply HashingTF (Term Frequency) to convert words into numerical form\n",
    "hashing_tf = HashingTF(inputCol=\"words\", outputCol=\"raw_features\", numFeatures=1000)\n",
    "train_df = hashing_tf.transform(train_df)\n",
    "val_df = hashing_tf.transform(val_df)\n",
    "test_df = hashing_tf.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "050ef985-bfa0-4e7f-8506-04bf45baee94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### TF-IDF Weighting with IDF Transformer\n",
    "\n",
    "This step applies the Inverse Document Frequency (IDF) transformation to our term frequency vectors:\n",
    "\n",
    "1. **IDF Calculation**: The IDF transformer weights term frequencies based on how rare or common words are across the entire document collection\n",
    "   \n",
    "2. **Model Fitting**: We fit the IDF model on the training data only to prevent data leakage from validation and test sets\n",
    "   \n",
    "3. **Feature Transformation**: The model is then applied to transform all datasets, creating the final \"features\" column\n",
    "\n",
    "This completes our TF-IDF (Term Frequency-Inverse Document Frequency) implementation, which gives higher weights to terms that are rare across documents but frequent within specific documents. This helps distinguish important, discriminative words from common, less informative ones.\n",
    "\n",
    "P.S. As we are having to run all of this locally for now I have trained the model on 10% of the dataset however for the actual thing you can just remove the extra portion in the cell block below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "556331e1-c249-466b-a640-d9d18020818d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 5: Apply IDF (Inverse Document Frequency) to weight the words\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "# Fit the IDF model using a .001% sample of the training data\n",
    "\n",
    "# .sample(fraction=0.01, seed=42)\n",
    "\n",
    "idf_model = idf.fit(train_df)\n",
    "\n",
    "# Transform all datasets using the fitted model\n",
    "train_df = idf_model.transform(train_df)\n",
    "val_df = idf_model.transform(val_df)\n",
    "test_df = idf_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83e20373-f30e-47a8-b538-f2b2117f5e51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature Engineering for Categories and Feature Assembly\n",
    "\n",
    "This section completes our feature engineering pipeline with three important steps:\n",
    "\n",
    "1. **Category Indexing**: Convert the text category labels to numerical indices\n",
    "   - We use StringIndexer to transform categorical text values into numeric indices\n",
    "   - This transformation is fitted only on the training dataset to avoid data leakage\n",
    "\n",
    "2. **One-Hot Encoding**: Transform numerical category indices into one-hot encoded vectors\n",
    "   - One-hot encoding creates binary vectors that machine learning algorithms can process\n",
    "   - Each category gets its own dimension in the feature space\n",
    "\n",
    "3. **Feature Assembly**: Combine all engineered features into a single vector\n",
    "   - We combine category features, normalized review length, and TF-IDF text features\n",
    "   - The VectorAssembler creates our final feature vectors (\"final_features\") for model training\n",
    "   - We verify the transformation by displaying samples of our processed data\n",
    "\n",
    "This completes our feature engineering pipeline, preparing the data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dfc9a07-635c-479c-be96-153290d88b5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Step 6: Convert the categorical 'category' column to numerical using StringIndexer \n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"category_index\")\n",
    "\n",
    "indexer_model = indexer.fit(train_df) \n",
    "\n",
    "train_df = indexer_model.transform(train_df)\n",
    "val_df = indexer_model.transform(val_df)\n",
    "test_df = indexer_model.transform(test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79b79a88-3598-4f18-b3e0-a01537b4474a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+\n",
      "|      final_features|category_onehot|            features|\n",
      "+--------------------+---------------+--------------------+\n",
      "|(1013,[0,12,30,61...| (12,[0],[1.0])|(1000,[17,48,61,6...|\n",
      "|(1013,[0,12,25,30...| (12,[0],[1.0])|(1000,[12,17,25,2...|\n",
      "|(1013,[0,12,19,25...| (12,[0],[1.0])|(1000,[6,12,17,25...|\n",
      "|(1013,[0,12,30,34...| (12,[0],[1.0])|(1000,[17,21,209,...|\n",
      "|(1013,[0,12,30,66...| (12,[0],[1.0])|(1000,[17,53,66,7...|\n",
      "+--------------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler\n",
    "\n",
    "# Step 7: Apply OneHotEncoder to the 'category_index' column (fit on train, transform on all)\n",
    "encoder = OneHotEncoder(inputCol=\"category_index\", outputCol=\"category_onehot\")\n",
    "encoder_model = encoder.fit(train_df)  # Fit on training data\n",
    "\n",
    "# Transform all datasets using the trained encoder model\n",
    "train_df = encoder_model.transform(train_df)\n",
    "val_df = encoder_model.transform(val_df)\n",
    "test_df = encoder_model.transform(test_df)\n",
    "\n",
    "# Step 8: Combine all features using VectorAssembler\n",
    "feature_cols = [\"category_onehot\", \"review_length_normalized\", \"features\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"final_features\")\n",
    "\n",
    "train_df = assembler.transform(train_df)\n",
    "val_df = assembler.transform(val_df)\n",
    "test_df = assembler.transform(test_df)\n",
    "\n",
    "# Check the transformed dataframe to confirm the changes\n",
    "train_df.select(\"final_features\", \"category_onehot\", \"features\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e050180-53cf-44cc-8ced-945c4a19359d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rf_base_path = \"wasbs://datafiles@aladdimbigdata.blob.core.windows.net/rf_data\"\n",
    "\n",
    "train_df.select(\"final_features\", \"overall\") \\\n",
    "    .write.mode(\"overwrite\").parquet(f\"{rf_base_path}/train\")\n",
    "\n",
    "val_df.select(\"final_features\", \"overall\") \\\n",
    "    .write.mode(\"overwrite\").parquet(f\"{rf_base_path}/val\")\n",
    "\n",
    "test_df.select(\"final_features\", \"overall\") \\\n",
    "    .write.mode(\"overwrite\").parquet(f\"{rf_base_path}/test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "344a32dc-68ed-4c50-9dce-cb4ccd676994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### **📌 Raw vs. Derived Features Table**\n",
    "| Feature Name               | Type         | Raw/Derived | Description |\n",
    "|----------------------------|-------------|------------|-------------|\n",
    "| `reviewText`               | Text        | Raw        | Original review text from dataset |\n",
    "| `category`                 | Categorical | Raw        | Product category of the review |\n",
    "| `overall`                   | Numeric     | Raw        | Target variable (1-5 stars) |\n",
    "| `review_length`            | Numeric     | Derived    | Length of the review text (character count) |\n",
    "| `review_length_normalized` | Numeric     | Derived    | Normalized length of the review |\n",
    "| `words`                    | List[String]| Derived    | Tokenized words from `reviewText` |\n",
    "| `raw_features`             | Vector      | Derived    | HashingTF applied to `words` (Term Frequency) |\n",
    "| `features`                 | Vector      | Derived    | IDF transformation on `raw_features` (TF-IDF representation) |\n",
    "| `category_index`           | Numeric     | Derived    | Indexed categorical value for `category` (StringIndexer applied) |\n",
    "| `category_onehot`          | Vector      | Derived    | OneHotEncoded vector for `category_index` |\n",
    "| `final_features`           | Vector      | Derived    | Combined feature vector (`features` + `category_onehot` + `review_length_normalized`) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbc21750-b0c7-4efe-8c53-846cfd3b0732",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Absence of Regularization and PCA\n",
    "\n",
    "Dimensionality Reduction:\n",
    "\n",
    "PCA: Thefeature set is not too large or correlated. From the current setup, dimensionality  is reduced with HashingTF (by specifying numFeatures=1000).\n",
    "\n",
    "LASSO: Applying L1 regularization (LASSO) in the logistic regression model can help with feature selection however as there are barely any there is no need for this\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ef53892-cda8-4dde-bd98-ce94ac409484",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Random Forest Model Training and Prediction\n",
    "\n",
    "This section implements the machine learning model using Random Forest classification:\n",
    "\n",
    "1. **Model Initialization**: \n",
    "   - We create a Random Forest Classifier with 10 decision trees\n",
    "   - The model will use our \"final_features\" column as input and predict the \"overall\" rating\n",
    "\n",
    "2. **Model Training**:\n",
    "   - We fit the Random Forest model on our prepared training data\n",
    "   - The model learns patterns between our engineered features and review ratings\n",
    "\n",
    "3. **Prediction Generation**:\n",
    "   - We apply the trained model to make predictions on all three datasets\n",
    "   - This creates new dataframes with prediction columns for evaluation\n",
    "   - Generating predictions on training data allows us to assess potential overfitting\n",
    "\n",
    "The Random Forest algorithm is well-suited for this task as it handles high-dimensional data effectively and can capture non-linear relationships in the feature space."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Milestone_2_random_forest_version_1",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
